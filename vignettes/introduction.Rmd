---
title: "Introduction"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(spartropy)
```

# Notes

* **All variables in this package are considered to be categorical (discrete) variables**
* Below $\log$ is used in a generic fashing -- any $\log$ with base $b$ goes
  + Later we see that in `spartropy` there are functions available for `log()` (natural logarithm), `log2()` and `log10()`
* Note that $0 \log 0 = 0$ by convention as this corresponds also to the limit


# Definitions and formulas

## Entropy, $\mathrm{H}$

Definition:

\begin{align}
  \mathrm{H}(X) &= - \sum_{x} P(X = x) \log P(X = x)
\end{align}
where the sum are over the possible values of $X$, denoted here by $x$.

Source: <https://en.wikipedia.org/wiki/Entropy_(information_theory)>

## Joint entropy, $\mathrm{H}(X, Y)$ (symmetric in $X$ and $Y$)

Definition:

\begin{align}
  \mathrm{H}(X , Y) 
  &= - \sum_{x, y} P(X = x , Y = y) \log_b P(X = x , Y = y) 
\end{align}
where the sum are over the possible values of $X$, denoted here by $x$, and over possible values of $Y$, denoted by $y$.

Source: <https://en.wikipedia.org/wiki/Joint_entropy>

## Conditional entropy, $\mathrm{H}(X \mid Y)$ (asymmetric in $X$ and $Y$)

Definition:

\begin{align}
  \mathrm{H}(X \mid Y) 
  &= - \sum_{x, y} P(X = x , Y = y) \log_b \frac{P(X = x , Y = y)}{P(Y = y)} 
\end{align}

Formulas:

\begin{align}
  \mathrm{H}(X \mid Y) &= \mathrm{H}(X, Y) - \mathrm{H}(Y) \\
  \mathrm{H}(Y \mid X) &= \mathrm{H}(X, Y) - \mathrm{H}(X) 
\end{align}

\begin{align}
  \mathrm{H}(X , Y) 
  &= \mathrm{H}(X \mid Y) + \mathrm{H}(Y) \\
  &= \mathrm{H}(Y \mid X) + \mathrm{H}(X) 
\end{align}

Source: <https://en.wikipedia.org/wiki/Conditional_entropy>

## Mutual information, $\operatorname{I} (X; Y)$ (symmetric in $X$ and $Y$)

Definition:

\begin{align}
  \operatorname{I} (X; Y)
  &= \sum_{x, y} P(X = x , Y = y) \log_b \frac{P(X = x , Y = y)}{P(X = x) P(Y = y)} 
\end{align}

Formulas:

\begin{align}
  \operatorname{I} (X; Y)
  &= \mathrm{H}(X) - \mathrm{H}(X \mid Y)\\ 
  &= \mathrm{H}(Y) - \mathrm{H}(Y \mid X)\\
  &= \mathrm{H}(X) + \mathrm{H}(Y) - \mathrm{H}(X,Y) \\
  &= \mathrm{H}(X,Y) - \mathrm{H}(X \mid Y) - \mathrm{H}(Y \mid X) \\
  \mathrm{H}(X \mid Y) &= \mathrm{H}(X) - \operatorname{I} (X; Y) 
\end{align}

Source: <https://en.wikipedia.org/wiki/Mutual_information>

## Shared information distance, $D(X, Y)$ (symmetric in $X$ and $Y$)

\begin{align}
  \operatorname{D} (X, Y)
  &= \frac{ \mathrm{H}(X \mid Y) + \mathrm{H}(Y \mid X) }{\mathrm{H}(X,Y)} 
\end{align}

$0 \leq \operatorname{D} (X, Y) \leq 1$ with $\operatorname{D} (X, Y) = 0$ iff 
$X$ and $Y$ are perfectly dependent (fully determined) and 
$\operatorname{D} (X, Y) = 1$ iff
$X$ and $Y$ are independent.


## Independence 

If $X$ and $Y$ are independent:

\begin{align}
  \mathrm{H} (Y \mid X) &= \mathrm {H}(Y) \\
  \mathrm{H} (X \mid Y) &= \mathrm {H}(X) .
\end{align}



# Functions

* `entropyB(d)` calculates entropy $H(X)$ and joint entropy $H(\ldots)$
* `entropy_condB(d, idx_x, idx_y)` calculates conditional entropy $H(X \mid Y)$ of `d[, idx_x]` **given** `d[, idx_y]`
* `mutinfB(d, idx_x, idx_y)` calculates mutual information between `d[, idx_x]` and `d[, idx_y]`

where `B` is either

* `E` for natural logarithm
* `2` for $\log2$
* `10` for $\log10$

# Example

Using `mtcars` data:

```{r}
head(mtcars)
```

## $H(\text{mpg})$

Entropy, $H$ of the `mpt` variable, i.e. $H(\text{mpg})$:

```{r}
H_mpg <- entropyE(mtcars[, "mpg"])
H_mpg
```

## $H(\text{mpg}, \text{hp}, \text{wt})$

```{r}
H_joint <- entropyE(mtcars[, c("mpg", "hp", "wt")])
H_joint
```

## $I(\text{mpg}; \text{hp}, \text{wt})$

```{r}
idx_x <- match("mpg", colnames(mtcars))
idx_y <- match(c("hp", "wt"), colnames(mtcars))
I_mpg_hpwt <- mutinfE(mtcars, idx_x, idx_y)
I_mpg_hpwt
```

## $H(\text{mpg} \mid \text{hp}, \text{wt})$

\begin{align}
  \mathrm{H}(X \mid Y) &= \mathrm{H}(X) - \operatorname{I} (X; Y) \\
  \mathrm{H}(\text{mpg} \mid \text{hp}, \text{wt}) 
  &= \mathrm{H}(\text{mpg}) - \operatorname{I} (\text{mpg}; \text{hp}, \text{wt}) \\
\end{align}

```{r}
H_mpg_hpwt <- H_mpg - I_mpg_hpwt
H_mpg_hpwt
entropy_condE(mtcars, idx_x, idx_y)
```

\begin{align}
  \mathrm{H}(\text{hp}, \text{wt} \mid \text{mpg}) 
  &= \mathrm{H}(\text{hp}, \text{wt}) - \operatorname{I} (\text{mpg}; \text{hp}, \text{wt}) \\
\end{align}

```{r}
H_hpwt <- entropyE(mtcars[, c("hp", "wt")])
H_hpwt_mpg <- H_hpwt - I_mpg_hpwt
H_hpwt_mpg
entropy_condE(mtcars, idx_y, idx_x)
```

\begin{align}
  \operatorname{D} (\text{mpg}, \{ \text{hp}, \text{wt} \})
  &= \frac{ \mathrm{H}(\text{mpg} \mid \text{hp}, \text{wt}) + 
    \mathrm{H}(\text{hp}, \text{wt} \mid \text{mpg}) }{\mathrm{H}(\text{mpg}, \text{hp}, \text{wt})} 
\end{align}

```{r}
D_mpg_hpwt <- (H_mpg_hpwt + H_hpwt_mpg) / H_joint
D_mpg_hpwt
(entropy_condE(mtcars, idx_x, idx_y) + entropy_condE(mtcars, idx_y, idx_x)) / entropyE(mtcars[, c("mpg", "hp", "wt")])
```

Thus, as $D$ is close to 0, then `hp` and `wt` says alot about `mpg` (and vica versa).

